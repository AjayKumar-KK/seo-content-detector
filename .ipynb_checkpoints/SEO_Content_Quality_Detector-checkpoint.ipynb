{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEO Content Quality & Duplicate Detector\n",
        "\n",
        "**Course / Assignment**: Data Science – SEO Content Quality & Duplicate Detection  \n",
        "**Student**: _<Your Name Here>_  \n",
        "**Program**: M.Sc. / M.Tech / MBA in Data Science / Analytics  \n",
        "**Notebook Goal**: Build an end-to-end SEO content analysis pipeline that:\n",
        "\n",
        "1. Parses raw HTML content and extracts clean text.\n",
        "2. Engineers SEO-related text features (length, readability, TF–IDF, etc.).\n",
        "3. Detects near-duplicate and thin content.\n",
        "4. Trains a content quality classifier (Low / Medium / High).\n",
        "5. Provides a real-time URL analysis demo that can be connected to a Streamlit UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment Setup & High-Level Design\n",
        "\n",
        "In this notebook, we follow the **assignment structure** and map each section to the corresponding requirement:\n",
        "\n",
        "1. **Data Collection & HTML Parsing (15%)**  \n",
        "2. **Text Preprocessing & Feature Engineering (25%)**  \n",
        "3. **Duplicate Detection & Thin Content (20%)**  \n",
        "4. **Content Quality Scoring (25%)**  \n",
        "5. **Real-Time Analysis Demo (15%)**  \n",
        "\n",
        "The raw dataset is stored locally on my machine at:\n",
        "\n",
        "```text\n",
        "/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/CodeWalnut/data.csv\n",
        "```\n",
        "\n",
        "This path is used only for reading; a project-local copy is saved under `./data/` to keep the repository portable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "from scipy.sparse import save_npz\n",
        "import pickle\n",
        "import requests\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Project directories\n",
        "# ------------------------------------------------------------------\n",
        "BASE_DIR = Path().resolve()\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Absolute path to the raw CSV on my machine\n",
        "ABS_INPUT_PATH = Path(\"/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/CodeWalnut/data.csv\")\n",
        "\n",
        "ABS_INPUT_PATH, DATA_DIR, MODELS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.1 Reusable Helper Functions\n",
        "\n",
        "To keep the notebook clean and modular, we define all text-processing, readability, and HTML-parsing utilities in one place and reuse them throughout the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Remove weird spaces and collapse multiple spaces.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.replace(\"\\xa0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "def sentence_tokenize(text: str):\n",
        "    \"\"\"Very simple sentence splitter based on punctuation.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
        "\n",
        "\n",
        "def word_tokenize(text: str):\n",
        "    \"\"\"Split text into alphanumeric tokens.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
        "\n",
        "\n",
        "def count_syllables(word: str) -> int:\n",
        "    \"\"\"Rough syllable counter – sufficient for Flesch score.\"\"\"\n",
        "    word = word.lower()\n",
        "    if len(word) <= 3:\n",
        "        return 1\n",
        "    vowels = \"aeiouy\"\n",
        "    count = 0\n",
        "    prev_vowel = False\n",
        "    for ch in word:\n",
        "        is_v = ch in vowels\n",
        "        if is_v and not prev_vowel:\n",
        "            count += 1\n",
        "        prev_vowel = is_v\n",
        "    if word.endswith(\"e\"):\n",
        "        count = max(1, count - 1)\n",
        "    return max(1, count)\n",
        "\n",
        "\n",
        "def flesch_reading_ease(text: str) -> float:\n",
        "    \"\"\"Compute Flesch Reading Ease score (higher = easier to read).\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sentence_tokenize(text)\n",
        "    if len(words) == 0 or len(sentences) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    syllables = sum(count_syllables(w) for w in words)\n",
        "    W = len(words)\n",
        "    S = len(sentences)\n",
        "    score = 206.835 - 1.015 * (W / S) - 84.6 * (syllables / W)\n",
        "    return round(score, 2)\n",
        "\n",
        "\n",
        "def extract_text_from_html(html: str):\n",
        "    \"\"\"\n",
        "    Extract title + main body text from HTML using BeautifulSoup.\n",
        "\n",
        "    Heuristic:\n",
        "    1) Remove <script>, <style>, <noscript>.\n",
        "    2) Prefer <main> or <article> blocks for the main content.\n",
        "    3) Fallback: join all <p> tags.\n",
        "    4) Last fallback: use the entire page text.\n",
        "    \"\"\"\n",
        "    if not isinstance(html, str) or not html.strip():\n",
        "        return \"\", \"\"\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    for t in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        t.extract()\n",
        "\n",
        "    title = soup.title.get_text(separator=\" \", strip=True) if soup.title else \"\"\n",
        "\n",
        "    candidates = []\n",
        "    for tag_name in [\"main\", \"article\"]:\n",
        "        tag = soup.find(tag_name)\n",
        "        if tag:\n",
        "            candidates.append(tag.get_text(separator=\" \", strip=True))\n",
        "\n",
        "    if not candidates:\n",
        "        ps = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        if len(ps) >= 2:\n",
        "            candidates.append(\" \".join(ps))\n",
        "        else:\n",
        "            candidates.append(soup.get_text(separator=\" \", strip=True))\n",
        "\n",
        "    body = max(candidates, key=len) if candidates else \"\"\n",
        "    return clean_text(title), clean_text(body)\n",
        "\n",
        "\n",
        "def label_quality(word_count: int, fre: float) -> str:\n",
        "    \"\"\"\n",
        "    Synthetic SEO-style quality label:\n",
        "    - High: long & comfortably readable\n",
        "    - Low: very short or very hard to read\n",
        "    - Medium: in-between\n",
        "    \"\"\"\n",
        "    if (word_count > 1500) and (50 <= fre <= 70):\n",
        "        return \"High\"\n",
        "    if (word_count < 500) or (fre < 30):\n",
        "        return \"Low\"\n",
        "    return \"Medium\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Collection & HTML Parsing (15%)\n",
        "\n",
        "In this section, I:\n",
        "\n",
        "1. Load the raw dataset from the given **absolute path** on my local machine.  \n",
        "2. Store a copy inside the project (`./data/data.csv`) for reproducibility and Git version control.  \n",
        "3. Parse the HTML stored in the `html_content` column to extract:\n",
        "   - Page `title`\n",
        "   - Cleaned `body_text` (main article content)\n",
        "   - `word_count` based on tokenized text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.1 Load raw dataset\n",
        "\n",
        "if not ABS_INPUT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"data.csv not found at: {ABS_INPUT_PATH}\")\n",
        "\n",
        "df_raw = pd.read_csv(ABS_INPUT_PATH)\n",
        "df_raw.columns = [c.strip().lower() for c in df_raw.columns]  # normalize\n",
        "\n",
        "print(\"Raw shape:\", df_raw.shape)\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.2 Basic sanity checks: schema + missingness\n",
        "\n",
        "display(df_raw.info())\n",
        "(df_raw.isna().mean() * 100).to_frame(\"missing_%\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.3 Save a project-local copy for reproducibility\n",
        "\n",
        "project_raw_path = DATA_DIR / \"data.csv\"\n",
        "df_raw.to_csv(project_raw_path, index=False)\n",
        "print(\"Copied raw data into:\", project_raw_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.4 HTML parsing → title, body_text, word_count\n",
        "\n",
        "if \"url\" not in df_raw.columns:\n",
        "    raise ValueError(\"Expected a 'url' column in the dataset.\")\n",
        "if \"html_content\" not in df_raw.columns:\n",
        "    raise ValueError(\"Expected an 'html_content' column with HTML source.\")\n",
        "\n",
        "titles, bodies, word_counts = [], [], []\n",
        "\n",
        "for _, row in df_raw.iterrows():\n",
        "    html = row.get(\"html_content\", \"\")\n",
        "    try:\n",
        "        title, body = extract_text_from_html(html)\n",
        "    except Exception:\n",
        "        title, body = \"\", \"\"\n",
        "    titles.append(title)\n",
        "    bodies.append(body)\n",
        "    word_counts.append(len(word_tokenize(body)))\n",
        "\n",
        "extracted = pd.DataFrame({\n",
        "    \"url\": df_raw[\"url\"],\n",
        "    \"title\": titles,\n",
        "    \"body_text\": bodies,\n",
        "    \"word_count\": word_counts\n",
        "})\n",
        "\n",
        "print(\"Extracted content shape:\", extracted.shape)\n",
        "extracted.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.5 Persist extracted content\n",
        "\n",
        "extracted_path = DATA_DIR / \"extracted_content.csv\"\n",
        "extracted.to_csv(extracted_path, index=False)\n",
        "extracted_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing & Feature Engineering (25%)\n",
        "\n",
        "Here I engineer a set of interpretable SEO features for each URL:\n",
        "\n",
        "- `word_count` – content length signal  \n",
        "- `sentence_count` – structural complexity  \n",
        "- `flesch_reading_ease` – readability score  \n",
        "- `is_thin` – thin content flag (`1` if word_count < 500)  \n",
        "- `quality_label` – synthetic quality tier (High / Medium / Low) based on SEO-inspired rules  \n",
        "- TF–IDF embeddings – for similarity and keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.1 Core features: sentence_count & readability\n",
        "\n",
        "sentence_counts = [len(sentence_tokenize(t)) for t in extracted[\"body_text\"]]\n",
        "readability_scores = [flesch_reading_ease(t) for t in extracted[\"body_text\"]]\n",
        "\n",
        "features = pd.DataFrame({\n",
        "    \"url\": extracted[\"url\"],\n",
        "    \"word_count\": extracted[\"word_count\"],\n",
        "    \"sentence_count\": sentence_counts,\n",
        "    \"flesch_reading_ease\": readability_scores\n",
        "})\n",
        "\n",
        "# Thin content flag\n",
        "features[\"is_thin\"] = (features[\"word_count\"] < 500).astype(int)\n",
        "\n",
        "# Synthetic quality label\n",
        "features[\"quality_label\"] = [\n",
        "    label_quality(wc, fre)\n",
        "    for wc, fre in zip(features[\"word_count\"], features[\"flesch_reading_ease\"])\n",
        "]\n",
        "\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.2 TF–IDF embeddings (for similarity + keywords)\n",
        "\n",
        "texts_for_tfidf = extracted[\"body_text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts_for_tfidf)\n",
        "\n",
        "X_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.3 Example: top 5 keywords for first 3 URLs\n",
        "\n",
        "def top_keywords_for_doc(idx, top_k=5):\n",
        "    row = X_tfidf[idx].toarray().ravel()\n",
        "    if row.sum() == 0:\n",
        "        return []\n",
        "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "    top_idx = row.argsort()[-top_k:][::-1]\n",
        "    return feature_names[top_idx].tolist()\n",
        "\n",
        "for i in range(min(3, X_tfidf.shape[0])):\n",
        "    print(\"URL:\", extracted.loc[i, \"url\"])\n",
        "    print(\"Top keywords:\", top_keywords_for_doc(i))\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.4 Persist features and embeddings\n",
        "\n",
        "features_path = DATA_DIR / \"features.csv\"\n",
        "features.to_csv(features_path, index=False)\n",
        "\n",
        "emb_path = DATA_DIR / \"tfidf_embeddings.npz\"\n",
        "save_npz(emb_path, X_tfidf)\n",
        "\n",
        "features_path, emb_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Duplicate Detection & Thin Content (20%)\n",
        "\n",
        "This section focuses on **content reuse and cannibalisation** within the site:\n",
        "\n",
        "1. Use TF–IDF embeddings to compute cosine similarity between every pair of pages.  \n",
        "2. Treat pairs with similarity ≥ 0.80 as **near-duplicates**.  \n",
        "3. Save all pairs into `data/duplicates.csv`.  \n",
        "4. Summarise:\n",
        "   - Total pages\n",
        "   - Number of duplicate pairs\n",
        "   - Thin content percentage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3.1 Cosine similarity matrix and duplicate pairs\n",
        "\n",
        "sim_matrix = cosine_similarity(X_tfidf)\n",
        "n = sim_matrix.shape[0]\n",
        "threshold = 0.8\n",
        "\n",
        "pairs = []\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        sim = sim_matrix[i, j]\n",
        "        if sim >= threshold:\n",
        "            pairs.append({\n",
        "                \"url1\": extracted.loc[i, \"url\"],\n",
        "                \"url2\": extracted.loc[j, \"url\"],\n",
        "                \"similarity\": round(float(sim), 4)\n",
        "            })\n",
        "\n",
        "duplicates_df = pd.DataFrame(pairs)\n",
        "duplicates_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3.2 Persist duplicate pairs + summary diagnostics\n",
        "\n",
        "dup_path = DATA_DIR / \"duplicates.csv\"\n",
        "duplicates_df.to_csv(dup_path, index=False)\n",
        "\n",
        "summary = {\n",
        "    \"total_pages\": int(len(extracted)),\n",
        "    \"duplicate_pairs\": int(len(duplicates_df)),\n",
        "    \"thin_pages\": int(features[\"is_thin\"].sum()),\n",
        "    \"thin_pct\": float(features[\"is_thin\"].mean() * 100)\n",
        "}\n",
        "\n",
        "dup_path, summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Content Quality Scoring (25%)\n",
        "\n",
        "We now build a **supervised classifier** that predicts the content quality tier (Low / Medium / High).\n",
        "\n",
        "- Target variable: `quality_label` (generated using SEO-style rules).  \n",
        "- Features:\n",
        "  - `word_count`\n",
        "  - `sentence_count`\n",
        "  - `flesch_reading_ease`\n",
        "- Model: `RandomForestClassifier` (strong baseline for tabular features).  \n",
        "- Baseline: a very simple *rule-only* classifier based on `word_count` thresholds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4.1 Prepare data and compute baseline performance\n",
        "\n",
        "X_model = features[[\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]].values\n",
        "y = features[\"quality_label\"].values\n",
        "\n",
        "# Baseline rule: only word_count\n",
        "baseline_pred = np.where(\n",
        "    features[\"word_count\"] > 1000,\n",
        "    \"High\",\n",
        "    np.where(features[\"word_count\"] < 500, \"Low\", \"Medium\")\n",
        ")\n",
        "\n",
        "baseline_acc = accuracy_score(y, baseline_pred)\n",
        "baseline_acc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4.2 Train/test split + RandomForest training\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_model, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "report = classification_report(\n",
        "    y_test, y_pred, labels=[\"Low\", \"Medium\", \"High\"], zero_division=0\n",
        ")\n",
        "\n",
        "acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4.3 Detailed evaluation & feature importances\n",
        "\n",
        "print(\"Baseline accuracy (word_count rule):\", baseline_acc)\n",
        "print(\"Model accuracy:\", acc)\n",
        "print(\"Model weighted F1:\", f1)\n",
        "print(\"\\nConfusion matrix (Low, Medium, High):\\n\", cm)\n",
        "print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "feature_names = [\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"\\nFeature importances:\")\n",
        "for name, imp in sorted(zip(feature_names, importances), key=lambda x: -x[1]):\n",
        "    print(f\"- {name}: {imp:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4.4 Persist trained model\n",
        "\n",
        "model_path = MODELS_DIR / \"quality_model.pkl\"\n",
        "with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(clf, f)\n",
        "\n",
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Real-Time Analysis Demo (15%)\n",
        "\n",
        "Finally, I provide a **real-time analysis function** that can be reused directly\n",
        "in a Streamlit app or an API endpoint.\n",
        "\n",
        "Given a URL, `analyze_url(url)` will:\n",
        "\n",
        "1. Fetch the HTML with a polite `User-Agent` and delay.  \n",
        "2. Extract clean text using the same parser as the offline pipeline.  \n",
        "3. Compute:\n",
        "   - `word_count`\n",
        "   - `sentence_count`\n",
        "   - `flesch_reading_ease`\n",
        "4. Generate:\n",
        "   - Rule-based quality label\n",
        "   - Model-based quality label\n",
        "   - Thin content flag\n",
        "5. Compute similarity against the existing corpus and return top similar URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/119.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def analyze_url(url: str, sim_threshold: float = 0.5, delay: float = 1.5):\n",
        "    \"\"\"\n",
        "    Real-time SEO analysis for a single URL.\n",
        "\n",
        "    Returns a dictionary containing:\n",
        "    - core metrics (length, readability)\n",
        "    - rule & model quality labels\n",
        "    - thin content flag\n",
        "    - list of similar pages from the dataset\n",
        "    \"\"\"\n",
        "    # 1) HTTP fetch\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        time.sleep(delay)\n",
        "        if resp.status_code != 200:\n",
        "            return {\"url\": url, \"error\": f\"HTTP {resp.status_code}\"}\n",
        "    except Exception as e:\n",
        "        return {\"url\": url, \"error\": str(e)}\n",
        "\n",
        "    # 2) HTML parsing\n",
        "    title, body = extract_text_from_html(resp.text)\n",
        "\n",
        "    # 3) Feature computation\n",
        "    wc = len(word_tokenize(body))\n",
        "    sc = len(sentence_tokenize(body))\n",
        "    fre = flesch_reading_ease(body)\n",
        "\n",
        "    rule_label = label_quality(wc, fre)\n",
        "\n",
        "    try:\n",
        "        X_new = np.array([[wc, sc, fre]])\n",
        "        model_label = clf.predict(X_new)[0]\n",
        "    except Exception:\n",
        "        model_label = rule_label\n",
        "\n",
        "    # 4) Similarity against corpus\n",
        "    new_vec = tfidf_vectorizer.transform([body])\n",
        "    sims = cosine_similarity(new_vec, X_tfidf).ravel()\n",
        "    top_idx = sims.argsort()[::-1][:10]\n",
        "\n",
        "    similar_pages = [\n",
        "        {\"url\": extracted.loc[i, \"url\"], \"similarity\": round(float(sims[i]), 4)}\n",
        "        for i in top_idx if sims[i] >= sim_threshold\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"word_count\": wc,\n",
        "        \"sentence_count\": sc,\n",
        "        \"readability\": fre,\n",
        "        \"rule_quality_label\": rule_label,\n",
        "        \"model_quality_label\": model_label,\n",
        "        \"is_thin\": wc < 500,\n",
        "        \"similar_pages\": similar_pages\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5.1 Demo call (use a crawl-friendly URL; some sites may return HTTP 403)\n",
        "\n",
        "test_url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
        "result = analyze_url(test_url)\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion & Next Steps\n",
        "\n",
        "This notebook demonstrates a complete **SEO content quality and duplicate detection pipeline**:\n",
        "\n",
        "- Parsed raw HTML into clean title and body text.\n",
        "- Engineered interpretable text features and thin-content flags.\n",
        "- Detected near-duplicate URLs via TF–IDF + cosine similarity.\n",
        "- Trained and evaluated a RandomForest-based quality classifier.\n",
        "- Exposed a reusable `analyze_url()` function suitable for a Streamlit front-end.\n",
        "\n",
        "**Possible extensions (for production):**\n",
        "- Replace TF–IDF with transformer-based sentence embeddings (e.g., Sentence-BERT) for semantic similarity.\n",
        "- Incorporate additional SEO signals (internal links, meta tags, heading structure, Core Web Vitals).\n",
        "- Persist results into a database and schedule periodic re-crawling for monitoring content drift."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}