{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db19d232-9a3a-4a7d-89a8-b24106da919c",
   "metadata": {},
   "source": [
    "# SEO Content Quality & Duplicate Detector\n",
    "\n",
    "This notebook implements the full pipeline:\n",
    "\n",
    "1. Load raw SEO dataset from CSV.\n",
    "2. Extract clean text from HTML (title + body).\n",
    "3. Engineer SEO-related features:\n",
    "   - word_count\n",
    "   - sentence_count\n",
    "   - Flesch Reading Ease (readability)\n",
    "4. Build TF-IDF representations for pages.\n",
    "5. Detect duplicate / near-duplicate URLs using cosine similarity.\n",
    "6. Create synthetic quality labels (High / Medium / Low).\n",
    "7. Train a RandomForest classifier to predict quality.\n",
    "8. Save processed files and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2bd236-b441-45a5-8dd2-2191262b41b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/data.csv'),\n",
       " PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data'),\n",
       " PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/models'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Imports and paths\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# Project directories (relative)\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Your absolute path to the raw CSV\n",
    "ABS_INPUT_PATH = Path(\"data/data.csv\")\n",
    "\n",
    "ABS_INPUT_PATH, DATA_DIR, MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e3ea4-acbd-4835-a465-88adde8fa635",
   "metadata": {},
   "source": [
    "## 2. Load dataset from absolute path\n",
    "\n",
    "We read the raw data from the given absolute path on the local machine and also\n",
    "save a copy into `./data/data.csv` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2466c0e-4f3f-41b1-af90-6a031cba1b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>html_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;!--[if lt IE 7]&gt; &lt;html class=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\\n    &lt;me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html data-unhead-vue-server-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                        html_content  \n",
       "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
       "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  \n",
       "2  <!DOCTYPE html><html data-unhead-vue-server-re...  \n",
       "3  \\n\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\"...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load raw dataset\n",
    "\n",
    "if not ABS_INPUT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"data.csv not found at: {ABS_INPUT_PATH}\")\n",
    "\n",
    "df_raw = pd.read_csv(ABS_INPUT_PATH)\n",
    "df_raw.columns = [c.strip().lower() for c in df_raw.columns]  # normalize columns\n",
    "\n",
    "print(df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa712e4-232a-4672-ad5f-c5ec7843aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/data.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save a copy inside the project data folder\n",
    "project_raw_path = DATA_DIR / \"data.csv\"\n",
    "df_raw.to_csv(project_raw_path, index=False)\n",
    "project_raw_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af31fb-93d9-4780-926b-d6e21014e239",
   "metadata": {},
   "source": [
    "## 3. Quick EDA\n",
    "\n",
    "Check basic statistics and missing values to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06785783-b340-42b5-95b4-18add810ac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81 entries, 0 to 80\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           81 non-null     object\n",
      " 1   html_content  69 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3e2bf0-1199-4bc5-a3cf-4c2772bbc2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url              0.000000\n",
       "html_content    14.814815\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.isna().mean() * 100  # percentage of missing values per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683edcd-cc96-4cf6-916c-3fa765d6afa3",
   "metadata": {},
   "source": [
    "## 4. Helper functions\n",
    "\n",
    "We define reusable text-processing and readability functions, plus HTML parsing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e82232-cb60-4acc-9f03-3e80d4a61c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Remove weird spaces and collapse multiple spaces.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def sentence_tokenize(text: str):\n",
    "    \"\"\"Very simple sentence splitter based on punctuation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "\n",
    "\n",
    "def word_tokenize(text: str):\n",
    "    \"\"\"Split text into alphanumeric tokens.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
    "\n",
    "\n",
    "def count_syllables(word: str) -> int:\n",
    "    \"\"\"Rough syllable counter – good enough for Flesch score.\"\"\"\n",
    "    word = word.lower()\n",
    "    if len(word) <= 3:\n",
    "        return 1\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    prev_vowel = False\n",
    "    for ch in word:\n",
    "        is_v = ch in vowels\n",
    "        if is_v and not prev_vowel:\n",
    "            count += 1\n",
    "        prev_vowel = is_v\n",
    "    if word.endswith(\"e\"):\n",
    "        count = max(1, count - 1)\n",
    "    return max(1, count)\n",
    "\n",
    "\n",
    "def flesch_reading_ease(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute Flesch Reading Ease score.\n",
    "    Higher = easier to read.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sentence_tokenize(text)\n",
    "    if len(words) == 0 or len(sentences) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    syllables = sum(count_syllables(w) for w in words)\n",
    "    W = len(words)\n",
    "    S = len(sentences)\n",
    "    score = 206.835 - 1.015 * (W / S) - 84.6 * (syllables / W)\n",
    "    return round(score, 2)\n",
    "\n",
    "\n",
    "def extract_text_from_html(html: str):\n",
    "    \"\"\"\n",
    "    Extract title + main body text from HTML using BeautifulSoup.\n",
    "    Heuristic:\n",
    "    1. Drop script/style/noscript.\n",
    "    2. Prefer <main> or <article>.\n",
    "    3. Fallback to concatenated <p> tags.\n",
    "    4. Last fallback: whole page text.\n",
    "    \"\"\"\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"\", \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for t in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        t.extract()\n",
    "\n",
    "    title = soup.title.get_text(separator=\" \", strip=True) if soup.title else \"\"\n",
    "\n",
    "    candidates = []\n",
    "    for tag_name in [\"main\", \"article\"]:\n",
    "        tag = soup.find(tag_name)\n",
    "        if tag:\n",
    "            candidates.append(tag.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    if not candidates:\n",
    "        ps = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        if len(ps) >= 2:\n",
    "            candidates.append(\" \".join(ps))\n",
    "        else:\n",
    "            candidates.append(soup.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    body = max(candidates, key=len) if candidates else \"\"\n",
    "    return clean_text(title), clean_text(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0844a7-d883-415a-8d52-877ca5e34ee2",
   "metadata": {},
   "source": [
    "## 5. Extract main content from HTML\n",
    "\n",
    "For each row, we parse the `html_content`, extract the `<title>` and the main body,\n",
    "and compute the raw `word_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f11170-759e-4af2-b9b7-8b504f97110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Blog Privacy &amp; Compliance Top 10 Cybersecurity...</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Home Insights Blog Posts 11 Cyber Defense Tips...</td>\n",
       "      <td>1085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cybersecurity Best Practices CISA provides inf...</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  word_count  \n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         332  \n",
       "1  Blog Privacy & Compliance Top 10 Cybersecurity...        1778  \n",
       "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1085  \n",
       "3  Cybersecurity Best Practices CISA provides inf...         844  \n",
       "4                                                              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "bodies = []\n",
    "word_counts = []\n",
    "\n",
    "for _, row in df_raw.iterrows():\n",
    "    html = row.get(\"html_content\", \"\")\n",
    "    title, body = extract_text_from_html(html)\n",
    "    titles.append(title)\n",
    "    bodies.append(body)\n",
    "    word_counts.append(len(word_tokenize(body)))\n",
    "\n",
    "extracted = pd.DataFrame({\n",
    "    \"url\": df_raw[\"url\"],\n",
    "    \"title\": titles,\n",
    "    \"body_text\": bodies,\n",
    "    \"word_count\": word_counts\n",
    "})\n",
    "\n",
    "extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea95737-894a-46fb-af64-3ba17eda56a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/extracted_content.csv')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_path = DATA_DIR / \"extracted_content.csv\"\n",
    "extracted.to_csv(extracted_path, index=False)\n",
    "extracted_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04b76e-acf4-4451-8ae4-9682f8af57d4",
   "metadata": {},
   "source": [
    "## 6. Feature engineering\n",
    "\n",
    "We compute:\n",
    "\n",
    "- sentence_count\n",
    "- flesch_reading_ease\n",
    "- thin content flag (`is_thin`)\n",
    "- a synthetic quality_label (High / Medium / Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c2649a-2bb9-4342-a92e-479ff11ccde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>is_thin</th>\n",
       "      <th>quality_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>332</td>\n",
       "      <td>7</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>1778</td>\n",
       "      <td>94</td>\n",
       "      <td>42.51</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>1085</td>\n",
       "      <td>72</td>\n",
       "      <td>56.26</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>844</td>\n",
       "      <td>27</td>\n",
       "      <td>-5.02</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  word_count  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog         332   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips        1778   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...        1085   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...         844   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...           0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  is_thin quality_label  \n",
       "0               7               -10.25        1           Low  \n",
       "1              94                42.51        0        Medium  \n",
       "2              72                56.26        0        Medium  \n",
       "3              27                -5.02        0           Low  \n",
       "4               0                 0.00        1           Low  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counts = [len(sentence_tokenize(t)) for t in extracted[\"body_text\"]]\n",
    "readability_scores = [flesch_reading_ease(t) for t in extracted[\"body_text\"]]\n",
    "\n",
    "def label_quality(word_count: int, fre: float) -> str:\n",
    "    \"\"\"\n",
    "    Simple rule-based quality label:\n",
    "    - High: long & comfortably readable\n",
    "    - Low: very short or very hard to read\n",
    "    - Medium: in-between\n",
    "    \"\"\"\n",
    "    if (word_count > 1500) and (50 <= fre <= 70):\n",
    "        return \"High\"\n",
    "    if (word_count < 500) or (fre < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\"\n",
    "\n",
    "features = pd.DataFrame({\n",
    "    \"url\": extracted[\"url\"],\n",
    "    \"word_count\": extracted[\"word_count\"],\n",
    "    \"sentence_count\": sentence_counts,\n",
    "    \"flesch_reading_ease\": readability_scores\n",
    "})\n",
    "\n",
    "features[\"is_thin\"] = (features[\"word_count\"] < 500).astype(int)\n",
    "features[\"quality_label\"] = [\n",
    "    label_quality(wc, fre) for wc, fre in zip(features[\"word_count\"], features[\"flesch_reading_ease\"])\n",
    "]\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5b69b2-ceb4-4295-86c6-dcc4108f798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/features.csv')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_path = DATA_DIR / \"features.csv\"\n",
    "features.to_csv(features_path, index=False)\n",
    "features_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2809001-dddc-45b7-b381-f1a5790c6499",
   "metadata": {},
   "source": [
    "## 7. TF-IDF embeddings & duplicate detection\n",
    "\n",
    "We represent each page using TF-IDF vectors on the `body_text` and\n",
    "compute cosine similarity. Pairs with similarity ≥ 0.80 are treated\n",
    "as potential duplicates / near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2747f89-ef04-42a0-a55a-da63d0d1914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_for_tfidf = extracted[\"body_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(texts_for_tfidf)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28cd03c4-901d-475a-a1fc-3c05d0c15d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/tfidf_embeddings.npz')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path = DATA_DIR / \"tfidf_embeddings.npz\"\n",
    "save_npz(emb_path, X_tfidf)\n",
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d8bce25-59e7-4dc6-af83-74824c910595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/SD-WAN</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>0.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.microsoft.com/en-us/security/busin...</td>\n",
       "      <td>https://www.zscaler.com/resources/security-ter...</td>\n",
       "      <td>0.8494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.forbes.com/advisor/business/what-i...</td>\n",
       "      <td>https://blog.hubspot.com/marketing/what-is-dig...</td>\n",
       "      <td>0.8278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url1  \\\n",
       "0               https://en.wikipedia.org/wiki/SD-WAN   \n",
       "1  https://www.microsoft.com/en-us/security/busin...   \n",
       "2  https://www.forbes.com/advisor/business/what-i...   \n",
       "\n",
       "                                                url2  similarity  \n",
       "0  https://www.fortinet.com/resources/cyberglossa...      0.8714  \n",
       "1  https://www.zscaler.com/resources/security-ter...      0.8494  \n",
       "2  https://blog.hubspot.com/marketing/what-is-dig...      0.8278  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix = cosine_similarity(X_tfidf)\n",
    "n = sim_matrix.shape[0]\n",
    "threshold = 0.8\n",
    "\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        sim = sim_matrix[i, j]\n",
    "        if sim >= threshold:\n",
    "            pairs.append({\n",
    "                \"url1\": extracted.loc[i, \"url\"],\n",
    "                \"url2\": extracted.loc[j, \"url\"],\n",
    "                \"similarity\": round(float(sim), 4)\n",
    "            })\n",
    "\n",
    "duplicates_df = pd.DataFrame(pairs)\n",
    "duplicates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4a2cbe0-6571-4cc1-bf94-bcc6516ffe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/duplicates.csv'),\n",
       " 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_path = DATA_DIR / \"duplicates.csv\"\n",
    "duplicates_df.to_csv(dup_path, index=False)\n",
    "dup_path, len(duplicates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22a15e0a-499c-436f-8152-b32489d7940a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/data/duplicates.csv'),\n",
       " {'total_pages': 81,\n",
       "  'duplicate_pairs': 3,\n",
       "  'thin_pages': 28,\n",
       "  'thin_pct': 34.5679012345679})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_path = DATA_DIR / \"duplicates.csv\"\n",
    "duplicates_df.to_csv(dup_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"total_pages\": int(len(extracted)),\n",
    "    \"duplicate_pairs\": int(len(duplicates_df)),\n",
    "    \"thin_pages\": int(features[\"is_thin\"].sum()),\n",
    "    \"thin_pct\": float(features[\"is_thin\"].mean() * 100)\n",
    "}\n",
    "\n",
    "dup_path, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c08338-0808-4a31-b573-1eca0db53210",
   "metadata": {},
   "source": [
    "## 8. Train quality classifier\n",
    "\n",
    "We train a RandomForest classifier to predict `quality_label` from:\n",
    "\n",
    "- word_count\n",
    "- sentence_count\n",
    "- flesch_reading_ease\n",
    "\n",
    "We also compare with a simple baseline that uses only `word_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "345a3e92-509b-483e-b2a0-02fd1f968d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49382716049382713"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model = features[[\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]].values\n",
    "y = features[\"quality_label\"].values\n",
    "\n",
    "# Baseline using only word_count (for your report)\n",
    "baseline_pred = np.where(\n",
    "    features[\"word_count\"] > 1000,\n",
    "    \"High\",\n",
    "    np.where(features[\"word_count\"] < 500, \"Low\", \"Medium\")\n",
    ")\n",
    "\n",
    "baseline_acc = accuracy_score(y, baseline_pred)\n",
    "baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ba4d0e-9027-4220-8bf0-2dc87c691c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96, 0.9543859649122808)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_model, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "report = classification_report(\n",
    "    y_test, y_pred, labels=[\"Low\", \"Medium\", \"High\"], zero_division=0\n",
    ")\n",
    "\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "910c3640-58b8-45dc-81fa-f181940236ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy (word_count rule): 0.49382716049382713\n",
      "Model accuracy: 0.96\n",
      "Model weighted F1: 0.9543859649122808\n",
      "Confusion matrix (Low, Medium, High):\n",
      " [[14  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  1  1]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      1.00      1.00        14\n",
      "      Medium       0.90      1.00      0.95         9\n",
      "        High       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.96        25\n",
      "   macro avg       0.97      0.83      0.87        25\n",
      "weighted avg       0.96      0.96      0.95        25\n",
      "\n",
      "\n",
      "Top features (by importance):\n",
      "- flesch_reading_ease: 0.443\n",
      "- sentence_count: 0.290\n",
      "- word_count: 0.267\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy (word_count rule):\", baseline_acc)\n",
    "print(\"Model accuracy:\", acc)\n",
    "print(\"Model weighted F1:\", f1)\n",
    "print(\"Confusion matrix (Low, Medium, High):\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", report)\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "print(\"\\nTop features (by importance):\")\n",
    "for name, imp in sorted(zip(feature_names, importances), key=lambda x: -x[1]):\n",
    "    print(f\"- {name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69d0966e-2d5d-48db-a81e-bee6dd9244e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/models/quality_model.pkl')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODELS_DIR / \"quality_model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c436c-320d-431a-92e3-aea357ea669a",
   "metadata": {},
   "source": [
    "# Real-Time Analysis Demo\n",
    "\n",
    "Requirements:\n",
    "- Implement `analyze_url(url)` function in this notebook.\n",
    "- For any given URL:\n",
    "  - Scrape the page with `requests`:\n",
    "    - Set a User-Agent header\n",
    "    - Add 1–2s delay (polite crawling)\n",
    "    - Handle errors (timeouts / non-200 responses)\n",
    "  - Extract main content using the same HTML parser.\n",
    "  - Compute features:\n",
    "    - word_count\n",
    "    - sentence_count\n",
    "    - readability (Flesch score)\n",
    "  - Predict:\n",
    "    - rule-based quality label\n",
    "    - model-based quality label (RandomForest)\n",
    "  - Compute similarity to existing dataset pages:\n",
    "    - Use TF-IDF vectorizer fitted earlier (`tfidf_vectorizer`)\n",
    "    - Return top similar URLs above a threshold (e.g., 0.5)\n",
    "- Return a Python dict and print it as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "562591b5-fa99-4407-a22f-882fbbc640c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Data_science\",\n",
      "  \"title\": \"Data science - Wikipedia\",\n",
      "  \"word_count\": 2698,\n",
      "  \"sentence_count\": 288,\n",
      "  \"readability\": 43.3,\n",
      "  \"rule_quality_label\": \"Medium\",\n",
      "  \"model_quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_pages\": [\n",
      "    {\n",
      "      \"url\": \"https://realpython.com/tutorials/data-science/\",\n",
      "      \"similarity\": 0.5444\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SEO-Detector/1.0; +https://example.com)\"\n",
    "}\n",
    "\n",
    "def analyze_url(url: str, sim_threshold: float = 0.5, delay: float = 1.5):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        time.sleep(delay)  # polite delay\n",
    "        if resp.status_code != 200:\n",
    "            return {\"url\": url, \"error\": f\"HTTP {resp.status_code}\"}\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    title, body = extract_text_from_html(resp.text)\n",
    "    wc = len(word_tokenize(body))\n",
    "    sc = len(sentence_tokenize(body))\n",
    "    fre = flesch_reading_ease(body)\n",
    "    rule_label = label_quality(wc, fre)\n",
    "\n",
    "    try:\n",
    "        X_new = np.array([[wc, sc, fre]])\n",
    "        model_label = clf.predict(X_new)[0]\n",
    "    except Exception:\n",
    "        model_label = rule_label\n",
    "\n",
    "    new_vec = tfidf_vectorizer.transform([body])\n",
    "    sims = cosine_similarity(new_vec, X_tfidf).ravel()\n",
    "    top_idx = sims.argsort()[::-1][:10]\n",
    "\n",
    "    similar_pages = [\n",
    "        {\"url\": extracted.loc[i, \"url\"], \"similarity\": round(float(sims[i]), 4)}\n",
    "        for i in top_idx if sims[i] >= sim_threshold\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"word_count\": wc,\n",
    "        \"sentence_count\": sc,\n",
    "        \"readability\": fre,\n",
    "        \"rule_quality_label\": rule_label,\n",
    "        \"model_quality_label\": model_label,\n",
    "        \"is_thin\": wc < 500,\n",
    "        \"similar_pages\": similar_pages\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "result = analyze_url(test_url)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6a537-cdea-446c-b9e3-aabb333980d2",
   "metadata": {},
   "source": [
    "# Streamlit App Integration\n",
    "\n",
    "In this final section, I generate a modular **Streamlit application** directly from the\n",
    "notebook. The app reuses:\n",
    "\n",
    "- the processed dataset (`data/extracted_content.csv`, `data/features.csv`)\n",
    "- the trained RandomForest model (`models/quality_model.pkl`)\n",
    "- the same feature engineering and quality rules as the notebook\n",
    "\n",
    "The code below creates the following structure:\n",
    "\n",
    "```text\n",
    "streamlit_app/\n",
    "├── app.py\n",
    "├── utils/\n",
    "│   ├── parser.py\n",
    "│   ├── features.py\n",
    "│   └── scorer.py\n",
    "└── models/\n",
    "    └── quality_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "359698c8-4bb1-4d85-89a2-0261bdf38bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folders:\n",
      "/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/streamlit_app\n",
      "/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/streamlit_app/utils\n",
      "/Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/streamlit_app/models\n",
      "Copied model → /Users/ajaykumark/Documents/UniArchive/MDS- Semesters/seo-content-detector/streamlit_app/models/quality_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Base is the same as earlier in the notebook\n",
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "# Create Streamlit app folders\n",
    "STREAMLIT_DIR = BASE_DIR / \"streamlit_app\"\n",
    "UTILS_DIR = STREAMLIT_DIR / \"utils\"\n",
    "STREAMLIT_MODELS_DIR = STREAMLIT_DIR / \"models\"\n",
    "\n",
    "STREAMLIT_DIR.mkdir(exist_ok=True)\n",
    "UTILS_DIR.mkdir(exist_ok=True)\n",
    "STREAMLIT_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Created folders:\")\n",
    "print(STREAMLIT_DIR)\n",
    "print(UTILS_DIR)\n",
    "print(STREAMLIT_MODELS_DIR)\n",
    "\n",
    "# Copy trained model into streamlit_app/models\n",
    "src_model = MODELS_DIR / \"quality_model.pkl\"   # created earlier in Section 4\n",
    "dst_model = STREAMLIT_MODELS_DIR / \"quality_model.pkl\"\n",
    "shutil.copy2(src_model, dst_model)\n",
    "\n",
    "print(\"Copied model →\", dst_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8539baa-a343-43c0-a66a-36d5b8997f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ parser.py created successfully → streamlit_app/utils/parser.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "parser_code = '''import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/119.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_and_parse(url: str, timeout: int = 10) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetch raw HTML from a URL. Returns HTML or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        if resp.status_code != 200:\n",
    "            return None\n",
    "        return resp.text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\\\xa0\", \" \")\n",
    "    return re.sub(r\"\\\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def extract_text_from_html(html: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract (title, body_text) from a raw HTML string using a simple heuristic.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for t in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        t.extract()\n",
    "\n",
    "    title = soup.title.get_text(separator=\" \", strip=True) if soup.title else \"\"\n",
    "\n",
    "    candidates = []\n",
    "    for tag_name in [\"main\", \"article\"]:\n",
    "        tag = soup.find(tag_name)\n",
    "        if tag:\n",
    "            candidates.append(tag.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    if not candidates:\n",
    "        ps = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        if len(ps) >= 2:\n",
    "            candidates.append(\" \".join(ps))\n",
    "        else:\n",
    "            candidates.append(soup.get_text(separator=\" \", strip=True))\n",
    "\n",
    "    body = max(candidates, key=len) if candidates else \"\"\n",
    "    return clean_text(title), clean_text(body)\n",
    "'''\n",
    "\n",
    "# save the corrected file\n",
    "UTILS_DIR = Path(\"streamlit_app/utils\")\n",
    "UTILS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(UTILS_DIR / \"parser.py\").write_text(parser_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ parser.py created successfully →\", UTILS_DIR / \"parser.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b4cc6a3-3611-49c6-84d0-e28e2aaff61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ features.py created successfully → streamlit_app/utils/features.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "features_code = '''import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def sentence_tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\\\s+\", text) if s.strip()]\n",
    "\n",
    "\n",
    "def word_tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
    "\n",
    "\n",
    "def count_syllables(word: str) -> int:\n",
    "    word = word.lower()\n",
    "    if len(word) <= 3:\n",
    "        return 1\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    prev_vowel = False\n",
    "    for ch in word:\n",
    "        is_v = ch in vowels\n",
    "        if is_v and not prev_vowel:\n",
    "            count += 1\n",
    "        prev_vowel = is_v\n",
    "    if word.endswith(\"e\"):\n",
    "        count = max(1, count - 1)\n",
    "    return max(1, count)\n",
    "\n",
    "\n",
    "def flesch_reading_ease(text: str) -> float:\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sentence_tokenize(text)\n",
    "    if len(words) == 0 or len(sentences) == 0:\n",
    "        return 0.0\n",
    "    syllables = sum(count_syllables(w) for w in words)\n",
    "    W = len(words)\n",
    "    S = len(sentences)\n",
    "    score = 206.835 - 1.015 * (W / S) - 84.6 * (syllables / W)\n",
    "    return round(score, 2)\n",
    "\n",
    "\n",
    "def embed_tfidf(texts, max_features: int = 5000):\n",
    "    \"\"\"\n",
    "    Fit TF-IDF on texts and return (X, vectorizer).\n",
    "    \"\"\"\n",
    "    texts = [t if isinstance(t, str) else \"\" for t in texts]\n",
    "    vec = TfidfVectorizer(max_features=max_features, stop_words=\"english\")\n",
    "    X = vec.fit_transform(texts)\n",
    "    return X, vec\n",
    "\n",
    "\n",
    "def top_keywords(vec: TfidfVectorizer, X_row, top_k: int = 5):\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    row = X_row.toarray().ravel()\n",
    "    if row.sum() == 0:\n",
    "        return []\n",
    "    idx = row.argsort()[-top_k:][::-1]\n",
    "    return feature_names[idx].tolist()\n",
    "'''\n",
    "\n",
    "# Write file\n",
    "UTILS_DIR = Path(\"streamlit_app/utils\")\n",
    "UTILS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(UTILS_DIR / \"features.py\").write_text(features_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ features.py created successfully →\", UTILS_DIR / \"features.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3883801-474d-4d05-9186-3b6427920a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written: streamlit_app/utils/scorer.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "scorer_code = '''import pickle\n",
    "\n",
    "\n",
    "def rule_quality_label(word_count: int, readability: float) -> str:\n",
    "    \"\"\"\n",
    "    Rule-based quality label (same logic as notebook).\n",
    "    \"\"\"\n",
    "    if (word_count > 1500) and (50 <= readability <= 70):\n",
    "        return \"High\"\n",
    "    if (word_count < 500) or (readability < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\"\n",
    "\n",
    "\n",
    "def load_quality_model(path):\n",
    "    \"\"\"\n",
    "    Load trained RandomForest model from disk.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "'''\n",
    "\n",
    "# Replace UTILS_DIR with your actual path if not already defined\n",
    "UTILS_DIR = Path(\"streamlit_app/utils\")\n",
    "UTILS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(UTILS_DIR / \"scorer.py\").write_text(scorer_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Written:\", UTILS_DIR / \"scorer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c80058f8-2fb0-494e-8922-33b17b7cc6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Make sure Python can find the local 'utils' package\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m APP_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(APP_DIR) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n\u001b[1;32m     14\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(APP_DIR))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Make sure Python can find the local 'utils' package\n",
    "# -----------------------------------------------------------\n",
    "APP_DIR = Path(__file__).resolve().parent\n",
    "if str(APP_DIR) not in sys.path:\n",
    "    sys.path.append(str(APP_DIR))\n",
    "\n",
    "from utils.parser import fetch_and_parse, extract_text_from_html\n",
    "from utils.features import (\n",
    "    sentence_tokenize,\n",
    "    word_tokenize,\n",
    "    flesch_reading_ease,\n",
    "    embed_tfidf,\n",
    ")\n",
    "from utils.scorer import load_quality_model, rule_quality_label\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Paths\n",
    "# -----------------------------------------------------------\n",
    "BASE_DIR = Path(__file__).resolve().parents[1]\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODEL_FILE = APP_DIR / \"models\" / \"quality_model.pkl\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Cached loaders\n",
    "# -----------------------------------------------------------\n",
    "@st.cache_data\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the preprocessed dataset produced by the Jupyter pipeline.\n",
    "    \"\"\"\n",
    "    extracted = pd.read_csv(DATA_DIR / \"extracted_content.csv\")\n",
    "    features = pd.read_csv(DATA_DIR / \"features.csv\")\n",
    "    return extracted, features\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Fit TF-IDF embeddings for similarity search.\n",
    "    \"\"\"\n",
    "    X, vec = embed_tfidf(list(texts))\n",
    "    return X, vec\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the trained RandomForest content quality model.\n",
    "    \"\"\"\n",
    "    return load_quality_model(MODEL_FILE)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Streamlit config\n",
    "# -----------------------------------------------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"SEO Content Quality & Duplicate Detector\",\n",
    "    page_icon=\"🔎\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "st.title(\"🔎 SEO Content Quality & Duplicate Detector\")\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "This dashboard builds on the Jupyter notebook pipeline to:\n",
    "\n",
    "- Analyse the quality of a single URL in real time.\n",
    "- Detect near-duplicate pages inside the dataset.\n",
    "- Summarise dataset-level SEO health (thin content, readability, labels).\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Load core artefacts\n",
    "extracted_df, features_df = load_dataset()\n",
    "X_tfidf, vectorizer = load_embeddings(\n",
    "    extracted_df[\"body_text\"].fillna(\"\").astype(str)\n",
    ")\n",
    "model = load_model()\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.header(\"Settings\")\n",
    "sim_threshold = st.sidebar.slider(\n",
    "    \"Similarity threshold (cosine)\",\n",
    "    min_value=0.5,\n",
    "    max_value=0.95,\n",
    "    value=0.8,\n",
    "    step=0.01,\n",
    ")\n",
    "st.sidebar.caption(\n",
    "    f\"Pairs with cosine similarity ≥ {sim_threshold:.2f} are treated as potential duplicates.\"\n",
    ")\n",
    "\n",
    "tab_analyze, tab_duplicates, tab_summary = st.tabs(\n",
    "    [\"📍 Analyze URL\", \"🧬 Duplicate Map\", \"📊 Dataset Summary\"]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Tab 1 – Analyze URL\n",
    "# -----------------------------------------------------------\n",
    "with tab_analyze:\n",
    "    st.subheader(\"1. Real-time SEO analysis for a URL\")\n",
    "\n",
    "    url = st.text_input(\n",
    "        \"Enter a URL\",\n",
    "        placeholder=\"https://en.wikipedia.org/wiki/Data_science\",\n",
    "    )\n",
    "\n",
    "    if st.button(\"Analyze URL\"):\n",
    "        if not url.strip():\n",
    "            st.warning(\"Please enter a URL.\")\n",
    "        else:\n",
    "            with st.spinner(\"Fetching and analyzing page...\"):\n",
    "                html = fetch_and_parse(url)\n",
    "                if html is None:\n",
    "                    st.error(\"Failed to fetch page (HTTP != 200 or network error).\")\n",
    "                else:\n",
    "                    # Parse and compute features\n",
    "                    title, body = extract_text_from_html(html)\n",
    "\n",
    "                    wc = len(word_tokenize(body))\n",
    "                    sc = len(sentence_tokenize(body))\n",
    "                    fre = flesch_reading_ease(body)\n",
    "\n",
    "                    rule_label = rule_quality_label(wc, fre)\n",
    "\n",
    "                    # Model prediction\n",
    "                    try:\n",
    "                        X_new = np.array([[wc, sc, fre]])\n",
    "                        model_label = model.predict(X_new)[0]\n",
    "                    except Exception:\n",
    "                        model_label = rule_label\n",
    "\n",
    "                    # Similarity to dataset\n",
    "                    new_vec = vectorizer.transform([body])\n",
    "                    sims = cosine_similarity(new_vec, X_tfidf).ravel()\n",
    "                    top_idx = sims.argsort()[::-1][:10]\n",
    "\n",
    "                    similar_rows = []\n",
    "                    for i in top_idx:\n",
    "                        if sims[i] >= sim_threshold:\n",
    "                            similar_rows.append(\n",
    "                                {\n",
    "                                    \"similar_url\": extracted_df.loc[i, \"url\"],\n",
    "                                    \"similarity\": round(float(sims[i]), 4),\n",
    "                                }\n",
    "                            )\n",
    "                    similar_df = pd.DataFrame(similar_rows)\n",
    "\n",
    "                    # KPIs\n",
    "                    c1, c2, c3 = st.columns(3)\n",
    "                    c1.metric(\"Word Count\", f\"{wc}\")\n",
    "                    c2.metric(\"Sentence Count\", f\"{sc}\")\n",
    "                    c3.metric(\"Readability (FRE)\", f\"{fre}\")\n",
    "\n",
    "                    st.markdown(\"### Quality Assessment\")\n",
    "                    st.write(f\"- **Rule-based label:** `{rule_label}`\")\n",
    "                    st.write(f\"- **Model prediction:** `{model_label}`\")\n",
    "                    st.write(f\"- **Thin content?** `{wc < 500}`\")\n",
    "\n",
    "                    st.markdown(\"### Similar Pages in Dataset\")\n",
    "                    if similar_df.empty:\n",
    "                        st.info(\n",
    "                            f\"No similar pages above similarity {sim_threshold:.2f}.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        st.dataframe(similar_df, use_container_width=True)\n",
    "\n",
    "                    st.markdown(\"### Extracted Content Preview\")\n",
    "                    st.caption(f\"Title: {title}\")\n",
    "                    st.write(body[:2000] + (\"...\" if len(body) > 2000 else \"\"))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Tab 2 – Duplicate Map\n",
    "# -----------------------------------------------------------\n",
    "with tab_duplicates:\n",
    "    st.subheader(\"2. Near-duplicate URLs in the dataset\")\n",
    "\n",
    "    with st.spinner(\"Computing cosine similarities...\"):\n",
    "        sim_matrix = cosine_similarity(X_tfidf)\n",
    "        n = sim_matrix.shape[0]\n",
    "        pairs = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                sim = sim_matrix[i, j]\n",
    "                if sim >= sim_threshold:\n",
    "                    pairs.append(\n",
    "                        {\n",
    "                            \"url_1\": extracted_df.loc[i, \"url\"],\n",
    "                            \"url_2\": extracted_df.loc[j, \"url\"],\n",
    "                            \"similarity\": round(float(sim), 4),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        dup_df = pd.DataFrame(pairs)\n",
    "\n",
    "    if dup_df.empty:\n",
    "        st.info(\n",
    "            f\"No duplicate pairs found with similarity ≥ {sim_threshold:.2f}. \"\n",
    "            \"Try lowering the threshold in the sidebar.\"\n",
    "        )\n",
    "    else:\n",
    "        st.write(\n",
    "            f\"Found **{len(dup_df)}** URL pairs with similarity ≥ {sim_threshold:.2f}.\"\n",
    "        )\n",
    "        st.dataframe(dup_df, use_container_width=True)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Tab 3 – Dataset Summary\n",
    "# -----------------------------------------------------------\n",
    "with tab_summary:\n",
    "    st.subheader(\"3. Dataset-level SEO summary\")\n",
    "\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Total Pages\", len(extracted_df))\n",
    "    col2.metric(\"Avg Word Count\", f\"{features_df['word_count'].mean():.0f}\")\n",
    "    col3.metric(\n",
    "        \"Thin Content (%)\",\n",
    "        f\"{(features_df['is_thin'].mean() * 100):.1f}%\",\n",
    "    )\n",
    "    col4.metric(\n",
    "        \"Readable (FRE 50–70) (%)\",\n",
    "        f\"{(features_df['flesch_reading_ease'].between(50, 70).mean() * 100):.1f}%\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### Sample of engineered features\")\n",
    "    st.dataframe(features_df.head(20), use_container_width=True)\n",
    "\n",
    "    st.markdown(\"#### Quality label distribution\")\n",
    "    st.bar_chart(features_df[\"quality_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828764b0-07d8-49a3-a259-d11e29845cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
